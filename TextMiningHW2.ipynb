{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextMiningHW2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WilliamSawran/ABC/blob/master/TextMiningHW2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "caY120_tSlg4",
        "outputId": "1b49c612-11bb-4624-da05-9185f883f105"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h7d4bxiDfCs4"
      },
      "source": [
        "!ls /content/drive/'My Drive'/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ic910DrZfjLT",
        "outputId": "7e30aa48-2e53-4e32-cffc-d0a6dc95428f"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "from os import listdir\n",
        "from os.path import isfile, join\n",
        "import pathlib\n",
        "\n",
        "#data_dir = pathlib.Path(\"/content/drive/'My Drive'/\").parent\n",
        "train_data_dir = pathlib.Path(\"/content/drive/My Drive/20_newsgroup/20news-bydate-train\")\n",
        "test_data_dir = pathlib.Path(\"/content/drive/My Drive/20_newsgroup/20news-bydate-test\")\n",
        "\n",
        "train_dirnames = os.listdir(train_data_dir)\n",
        "print(\"Training Data:\")\n",
        "print(\"Number of directories:\", len(train_dirnames))\n",
        "print(\"Directory names:\", train_dirnames)\n",
        "filenames = os.listdir(train_data_dir / \"rec.autos\")\n",
        "print(\"\\tNumber of files in rec.autos:\", len(filenames))\n",
        "print(\"\\tSome example filenames:\", filenames[:5])\n",
        "filenames = os.listdir(train_data_dir / \"comp.sys.mac.hardware\")\n",
        "print(\"\\tNumber of files in comp.sys.mac.hardware:\", len(filenames))\n",
        "print(\"\\tSome example filenames:\", filenames[:5])\n",
        "\n",
        "test_dirnames = os.listdir(test_data_dir)\n",
        "print(\"\\nTesting Data:\")\n",
        "print(\"Number of directories:\", len(test_dirnames))\n",
        "print(\"Directory names:\", test_dirnames)\n",
        "filenames = os.listdir(test_data_dir / \"rec.autos\")\n",
        "print(\"\\tNumber of files in rec.autos:\", len(filenames))\n",
        "print(\"\\tSome example filenames:\", filenames[:5])\n",
        "filenames = os.listdir(test_data_dir / \"comp.sys.mac.hardware\")\n",
        "print(\"\\tNumber of files in comp.sys.mac.hardware:\", len(filenames))\n",
        "print(\"\\tSome example filenames:\", filenames[:5])\n",
        "\n",
        "print(\"\\ntrain_folders\")\n",
        "train_folders = [f for f in train_dirnames]\n",
        "print(train_folders)\n",
        "print(\"\\ntest_folders\")\n",
        "test_folders = [f for f in test_dirnames]\n",
        "print(test_folders)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Data:\n",
            "Number of directories: 2\n",
            "Directory names: ['comp.sys.mac.hardware', 'rec.autos']\n",
            "\tNumber of files in rec.autos: 594\n",
            "\tSome example filenames: ['101595', '101598', '101577', '101593', '101562']\n",
            "\tNumber of files in comp.sys.mac.hardware: 578\n",
            "\tSome example filenames: ['50428', '50464', '50458', '50483', '50435']\n",
            "\n",
            "Training Data:\n",
            "Number of directories: 2\n",
            "Directory names: ['rec.autos', 'comp.sys.mac.hardware']\n",
            "\tNumber of files in rec.autos: 396\n",
            "\tSome example filenames: ['103270', '103037', '103439', '103417', '103339']\n",
            "\tNumber of files in comp.sys.mac.hardware: 385\n",
            "\tSome example filenames: ['51965', '51984', '51982', '51997', '51978']\n",
            "\n",
            "train_folders\n",
            "['comp.sys.mac.hardware', 'rec.autos']\n",
            "\n",
            "test_folders\n",
            "['rec.autos', 'comp.sys.mac.hardware']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "msZV2qO2AWMa"
      },
      "source": [
        "#creating a 2D list to store list of all files in different folders\n",
        "\n",
        "train_files = []\n",
        "for train_folder_name in train_folders:\n",
        "  train_folder_path = join(train_data_dir, train_folder_name)\n",
        "  for f in listdir(train_folder_path):\n",
        "    train_file_path = join(train_data_dir, train_folder_name, f)\n",
        "    train_files.append(train_file_path)\n",
        "\n",
        "test_files = []\n",
        "for test_folder_name in test_folders:\n",
        "  test_folder_path = join(test_data_dir, test_folder_name)\n",
        "  for f in listdir(test_folder_path):\n",
        "    test_file_path = join(test_data_dir, test_folder_name, f)\n",
        "    test_files.append(test_file_path)\n",
        "\n"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LNurwiuZAd3N",
        "outputId": "a48a4f00-da61-4022-e832-af3beac07d31"
      },
      "source": [
        "#checking total no. of files gathered\n",
        "print(len(train_files))\n",
        "\n",
        "#checking total no. of files gathered\n",
        "print(len(test_files))"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1172\n",
            "781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdMinJVpDcxa",
        "outputId": "3cbec99a-3c30-4a4b-b42d-5c4c7ac3a7ad"
      },
      "source": [
        "#making an array containing the classes each of the documents belong to\n",
        "train_Y = []\n",
        "for train_folder_name in train_folders:\n",
        "  train_folder_path = join(train_data_dir, train_folder_name)\n",
        "  num_of_files= len(listdir(train_folder_path))\n",
        "  for i in range(num_of_files):\n",
        "    train_Y.append(train_folder_name)\n",
        "\n",
        "#making an array containing the classes each of the documents belong to\n",
        "test_Y = []\n",
        "for test_folder_name in test_folders:\n",
        "  test_folder_path = join(test_data_dir, test_folder_name)\n",
        "  num_of_files= len(listdir(test_folder_path))\n",
        "  for i in range(num_of_files):\n",
        "    test_Y.append(test_folder_name)\n",
        "\n",
        "print(len(train_Y))\n",
        "print(len(test_Y))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1172\n",
            "781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxkGn-DfEivC"
      },
      "source": [
        "# splitting the data into train test."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zL1SKzx8EfJh"
      },
      "source": [
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "45iszepnFY_X"
      },
      "source": [
        "#a simple helper function to convert a 2D array to 1D, without using numpy\n",
        "def flatten(list):\n",
        "  new_list = []\n",
        "  for i in list:\n",
        "    for j in i:\n",
        "      new_list.append(j)\n",
        "  return new_list"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5IuZbGDrgUqj"
      },
      "source": [
        "!python -m spacy download en_core_web_lg\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3wfPl4JExS-"
      },
      "source": [
        "import spacy\n",
        "import string\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from spacy import displacy\n",
        "from collections import Counter\n",
        "#import en_core_web_lg\n",
        "nlp = spacy.load('en_core_web_lg')\n",
        "\n",
        "\n",
        "list_of_words = []\n",
        "\n",
        "for document in train_files:\n",
        "  #load document\n",
        "  text = open(document, 'r', encoding=\"ISO-8859-1\")\n",
        "  text = BeautifulSoup(text, 'html.parser').get_text()\n",
        "  text = re.sub(r'…', '...', text)\n",
        "  text = re.sub(r'[`‘’‛⸂⸃⸌⸍⸜⸝]', \"'\", text)\n",
        "  text = re.sub(r'[„“]|(\\'\\')|(,,)', '\"', text)\n",
        "  text = re.sub(r'\\|>', '', text)\n",
        "  #text = re.sub(r'\\s+', ' ', text).strip()\n",
        "  #text = re.sub(r'[^A-Za-z0-9 ]+', '', text)\n",
        "  document = nlp(text)\n",
        "  #print(document)\n",
        "  tokens = []\n",
        "  for sent in document.sents:\n",
        "      for token in sent:\n",
        "        if token.is_alpha and len(token)>2 and not token.is_stop:\n",
        "          #if token.IS_PUNCT and if not token.is_stop::\n",
        "          tokens.append(token.lemma_.lower())\n",
        "  list_of_words.append(flatten(tokens))\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IErwa23dD6mU",
        "outputId": "a8d352af-d6b8-4c71-8ed4-41534a5b8515"
      },
      "source": [
        "len(list_of_words)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1172"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrJrLxiqEPSU",
        "outputId": "ae1a90f8-3d20-4920-af50-5ca90f4291d2"
      },
      "source": [
        "len(flatten(list_of_words))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "620061"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1wxi3eY1FusV",
        "outputId": "0191e4fd-a261-4665-fbb7-5434589fef21"
      },
      "source": [
        "import numpy as np\n",
        "np_list_of_words = np.asarray(flatten(list_of_words))\n",
        "len(np_list_of_words)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "620061"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2rYWeLjlG8FV",
        "outputId": "9d7277b4-9035-401f-d26b-7788c7362ad5"
      },
      "source": [
        "#finding the number of unique words that we have extracted from the documents\n",
        "words, counts = np.unique(np_list_of_words, return_counts=True)\n",
        "len(words)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "32"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VUbgtGbkhllb"
      },
      "source": [
        "#sorting the unique words according to their frequency\n",
        "freq, wrds = (list(i) for i in zip(*(sorted(zip(counts, words), reverse=True))))"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h064AbEPF1mK"
      },
      "source": [
        "#deciding the no. of words to use as feature\n",
        "n = 5000\n",
        "features = wrds[0:n]"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ghK0278Tjywq"
      },
      "source": [
        "#creating a dictionary that contains each document's vocabulary and ocurence of each word of the vocabulary\n",
        "dictionary = {}\n",
        "doc_num = 1\n",
        "for doc_words in list_of_words:\n",
        "  #print(doc_words)\n",
        "  np_doc_words = np.asarray(doc_words)\n",
        "  w, c = np.unique(np_doc_words, return_counts=True)\n",
        "  dictionary[doc_num] = {}\n",
        "  for i in range(len(w)):\n",
        "    dictionary[doc_num][w[i]] = c[i]\n",
        "  doc_num = doc_num + 1"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2wTqbX7FkG1v"
      },
      "source": [
        "#now we make a 2D array having the frequency of each word of our feature set in each individual documents\n",
        "X_train = []\n",
        "for k in dictionary.keys():\n",
        "  row = []\n",
        "  for f in features:\n",
        "    if(f in dictionary[k].keys()):\n",
        "      #if word f is present in the dictionary of the document as a key, its value is copied\n",
        "      #this gives us no. of occurences\n",
        "      row.append(dictionary[k][f])\n",
        "    else:\n",
        "      #if not present, the no. of occurences is zero\n",
        "      row.append(0)\n",
        "  X_train.append(row)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B33qIZiokppO"
      },
      "source": [
        "#we convert the X and Y into np array for concatenation and conversion into\n",
        "dataframe\n",
        "X_train = np.asarray(X_train)\n",
        "train_Y = np.asarray(train_Y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eqz6lhv6lXw3",
        "outputId": "9ae2d05d-b3bf-44ea-ee09-d2a06b654260"
      },
      "source": [
        "len(X_train)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1172"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F5gf9fFOliTr",
        "outputId": "66fadc5c-cce7-4e46-b254-8865c4f5dec3"
      },
      "source": [
        "len(train_Y)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1172"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUvivGDeeuXM"
      },
      "source": [
        "# we'll make our test data by performing the same operations as we did for train data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fBLDa-LSl-9i"
      },
      "source": [
        "list_of_words_test = []\n",
        "\n",
        "for document in test_files:\n",
        "  #load document\n",
        "  text = open(document, 'r', encoding=\"ISO-8859-1\")\n",
        "  text = BeautifulSoup(text, 'html.parser').get_text()\n",
        "  text = re.sub(r'…', '...', text)\n",
        "  text = re.sub(r'[`‘’‛⸂⸃⸌⸍⸜⸝]', \"'\", text)\n",
        "  text = re.sub(r'[„“]|(\\'\\')|(,,)', '\"', text)\n",
        "  text = re.sub(r'\\|>', '', text)\n",
        "  #text = re.sub(r'\\s+', ' ', text).strip()\n",
        "  #text = re.sub(r'[^A-Za-z0-9 ]+', '', text)\n",
        "  document = nlp(text)\n",
        "  #print(document)\n",
        "  tokens = []\n",
        "  for sent in document.sents:\n",
        "      for token in sent:\n",
        "        if token.is_alpha and len(token)>2 and not token.is_stop:\n",
        "          #if token.IS_PUNCT and if not token.is_stop::\n",
        "          tokens.append(token.lemma_.lower())\n",
        "  list_of_words_test.append(flatten(tokens))"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DJdv1wlbqMHR"
      },
      "source": [
        "dictionary_test = {}\n",
        "doc_num = 1\n",
        "for doc_words in list_of_words_test:\n",
        "  #print(doc_words)\n",
        "  np_doc_words = np.asarray(doc_words)\n",
        "  w, c = np.unique(np_doc_words, return_counts=True)\n",
        "  dictionary_test[doc_num] = {}\n",
        "  for i in range(len(w)):\n",
        "    dictionary_test[doc_num][w[i]] = c[i]\n",
        "  doc_num = doc_num + 1"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eKviIOQ1qXeG"
      },
      "source": [
        "#now we make a 2D array having the frequency of each word of our feature set in each individual documents\n",
        "X_test = []\n",
        "for k in dictionary_test.keys():\n",
        "  row = []\n",
        "  for f in features:\n",
        "    if(f in dictionary_test[k].keys()):\n",
        "      #if word f is present in the dictionary of the document as a key, its value is copied\n",
        "      #this gives us no. of occurences\n",
        "        row.append(dictionary_test[k][f])\n",
        "    else:\n",
        "      #if not present, the no. of occurences is zero\n",
        "      row.append(0)\n",
        "  X_test.append(row)"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkA1iUKVq6RC"
      },
      "source": [
        "X_test = np.asarray(X_test)\n",
        "test_Y = np.asarray(test_Y)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xmFP4dr9rFsp",
        "outputId": "0a9b7d1e-5e8d-4e86-936d-241a4771b4a4"
      },
      "source": [
        "len(X_test)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "781"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jtkqM3RNrH_3",
        "outputId": "a8a8cd62-ab3e-4fea-c957-94ec001d24c3"
      },
      "source": [
        "len(test_Y)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "781"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX44hPiIgM2y"
      },
      "source": [
        "# performing Text Classification using sklearn's Multinomial Naive Bayes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dr7n3qh6rMU3",
        "outputId": "f6c4ed3a-cc8b-445c-b599-20ab75f8c808"
      },
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB()\n",
        "clf.fit(X_train, train_Y)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHZpaGsMrZJ2"
      },
      "source": [
        "Y_predict = clf.predict(X_test)"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DAIG76_Drcea",
        "outputId": "6727b95b-0efd-4342-c31c-a76a022732b7"
      },
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
        "print(classification_report(test_Y, Y_predict))"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                       precision    recall  f1-score   support\n",
            "\n",
            "comp.sys.mac.hardware       0.77      0.66      0.71       385\n",
            "            rec.autos       0.71      0.80      0.75       396\n",
            "\n",
            "             accuracy                           0.73       781\n",
            "            macro avg       0.74      0.73      0.73       781\n",
            "         weighted avg       0.74      0.73      0.73       781\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cIxR83aEgSXK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eAuv_RdhgVGi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GjBxpAfLgkj8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tYVaalYp8XFb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MkTzA3lAYk8V"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}